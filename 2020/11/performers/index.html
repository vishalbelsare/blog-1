<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Performers: The Kernel Trick, Random Features, and Attention | Teddy Koker</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Performers: The Kernel Trick, Random Features, and Attention" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="THIS IS A DRAFT. CHECK AGAIN TOMORROW OR CONTINUE AT YOUR OWN RISK." />
<meta property="og:description" content="THIS IS A DRAFT. CHECK AGAIN TOMORROW OR CONTINUE AT YOUR OWN RISK." />
<link rel="canonical" href="https://teddykoker.com/2020/11/performers/" />
<meta property="og:url" content="https://teddykoker.com/2020/11/performers/" />
<meta property="og:site_name" content="Teddy Koker" />
<meta property="og:image" content="https://teddykoker.com/images/iid_vs_ortho.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-11T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://teddykoker.com/images/iid_vs_ortho.png" />
<meta property="twitter:title" content="Performers: The Kernel Trick, Random Features, and Attention" />
<meta name="twitter:site" content="@teddykoker" />
<script type="application/ld+json">
{"url":"https://teddykoker.com/2020/11/performers/","dateModified":"2020-11-11T00:00:00-05:00","datePublished":"2020-11-11T00:00:00-05:00","headline":"Performers: The Kernel Trick, Random Features, and Attention","mainEntityOfPage":{"@type":"WebPage","@id":"https://teddykoker.com/2020/11/performers/"},"description":"THIS IS A DRAFT. CHECK AGAIN TOMORROW OR CONTINUE AT YOUR OWN RISK.","image":"https://teddykoker.com/images/iid_vs_ortho.png","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://teddykoker.com/feed.xml" title="Teddy Koker" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-138897125-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<link rel="shortcut icon" href="/favicon.png">

  <!-- Katex Math (use defer to speed page load) -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
        integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq"
        crossorigin="anonymous">

  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
          integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
          crossorigin="anonymous"></script>

  <!--
  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mathtex-script-type.min.js"
          integrity="sha384-LJ2FmexL77rmGm6SIpxq7y+XA6bkLzGZEgCywzKOZG/ws4va9fUVu2neMjvc3zdv"
          crossorigin="anonymous" ></script> -->

  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
          integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
          crossorigin="anonymous"
          onload='renderMathInElement(document.body,{delimiters: [{left: "\\[",
          right: "\\]", display: true}, {left: "$", right: "$", display: false}]})'></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Teddy Koker</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/resume.pdf" onclick="ga('send', 'event', 'pdf', 'download', 'resume')">Résumé</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Performers: The Kernel Trick, Random Features, and Attention</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-11-11T00:00:00-05:00" itemprop="datePublished">Nov 11, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>THIS IS A DRAFT. CHECK AGAIN TOMORROW OR CONTINUE AT YOUR OWN RISK.</p>

<p>Google AI recently released a paper, <em>Rethinking Attention with Performers</em>
<a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a>, which introduces <em>Performer</em>, a Transformer
architecture which estimates the full-rank-attention mechanism using orthogonal
random features to approximate the softmax kernel with linear space and time
complexity. In this post we will investigate how this works, and how it is
useful for the machine learning community.</p>

<!--more-->

<h2 id="the-kernel-trick">The Kernel Trick</h2>

<p>Before we talk about Attention, it is important to understand the <a href="https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick">kernel
trick</a>.
Gregory Gundersen gives a great explanation on his
<a href="http://gregorygundersen.com/blog/2019/12/10/kernel-trick/">blog</a>, but we will go over a
brief summary here.</p>

<p>Say we have some data $x \in \mathbb{R}^2$:</p>

<p><img src="/images/circles.png" alt="circles" /></p>

<p>We can see that this data is not linearly separable, i.e. if we wanted to fit a
logistic regression or linear support vector machine (SVM) to the data we
wouldn’t be able to. How do we get around this? We can map the data into a
higher dimension, say $\mathbb{R}^3$, using a function 
$\varphi : \mathbb{R}^2 \mapsto \mathbb{R}^3$:</p>

\[\varphi \left(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \right)
 = \begin{bmatrix} x_1^2 \\ x_2^2 \\ \sqrt{2}x_1 x_2 \end{bmatrix}\]

<p>This function $\varphi$ is known as the <a href="https://en.wikipedia.org/wiki/Polynomial_kernel">polynomial
kernel</a> of degree $d=2$. If we
apply it to our data, $x$, we can visualize that it becomes linearly separable:</p>

<p><img src="/images/poly_circles.png" alt="poly circles" /></p>

<p>Now we can recall the expression for fitting a linear SVM:</p>

\[\text{maximize} \quad f(c) = \sum_n^N c_n - \frac{1}{2} 
\sum_i^N \sum_j^N c_i c_j y_i y_j (\textcolor{blue}{x_i^\top x_j})\]

<p>If this expression is unfamiliar to you, just note the dot-product of our data,
$\textcolor{blue}{x_i^\top x_j}$. If we want to use our kernel function $\varphi$
now, we can simple wrap each $x_n$ with it like so:</p>

\[\text{maximize} \quad f(c) = \sum_n^N c_n - \frac{1}{2} 
\sum_i^N \sum_j^N c_i c_j y_i y_j (\textcolor{blue}{\varphi(x_i)^\top \varphi(x_j)})\]

<p>Now we could stop here, but we can make this more efficient. What we have so far
requires computing $\varphi(x_n)$, $N$ times, and the dot-product
$\varphi(x_i)^\top \varphi(x_j)$, $N^2$ times, which could start becoming very
computationally expensive, especially with kernel functions that map to very
high dimensions. How do we get around this? This is where the <strong>kernel trick</strong> comes
in. Suppose we had a function $K : \mathbb{R}^2 \times \mathbb{R}^2 \mapsto \mathbb{R}$
where:</p>

\[K(x_i, x_j) = \varphi(x_i)^\top \varphi(x_j)\]

<p>If we can find a $K$ that performs this operation in a lower dimensional space,
we can save potentially great amounts of compute. For the polynomial kernel
$\varphi$ defined above, finding $K$ is fairly straight forward (derivation is
left as an exercise to the reader):</p>

\[K(x_i, x_j) = (x_i^\top x_j)^2 = \varphi(x_i)^\top \varphi(x_j)\]

<p>We are now doing the dot-product in lower dimensional space, but we will get the
same result as performing the dot-product after the projection. This means we
can rewrite our linear SVM expression one more time:</p>

\[\text{maximize} \quad f(c) = \sum_n^N c_n - \frac{1}{2} 
\sum_i^N \sum_j^N c_i c_j y_i y_j \textcolor{blue}{K(x_i, x_j)}\]

<h2 id="random-fourier-features">Random Fourier Features</h2>

<p>In <em>Random Features for Large-Scale Kernel Machines</em> <a class="citation" href="#rahimi2007random">(Rahimi &amp; Recht, 2007)</a>
(which won the NIPS “Test of Time” award in 2017, ten years after it was
published), they set out to approximate $K$ using a randomized feature map 
$z: \mathbb{R}^L \mapsto \mathbb{R}^R$:</p>

\[K(x_i, x_j) = \varphi(x_i)^\top \varphi(x_j) \approx z(x_i)^\top z(x_j)\]

<p>Specifically, they prove theoretically that the Gaussian or <a href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">RBF
kernel</a>:</p>

\[K_\text{gauss}(x_i, x_j) = \exp(-\gamma \lVert x_i - x_j \rVert^2)\]

<p>Can be approximated by sampling $z$ from the Fourier transformation. Concretely,
one way we can write this as is:</p>

\[z_\omega(x) = \begin{bmatrix} \cos(\omega^\top x) \\ 
\sin(\omega^\top x) \end{bmatrix}\]

<p>Where $\omega \sim \mathcal{N}_ R(0, I) $ is sampled from a spherical Gaussian.
With this approximation, the dot-product no longer needs to be performed in
$\mathbb{R}^L$ space, it can now be performed in $\mathbb{R}^R$ space, where
$R \ll L$. How is this useful? <strong>Using Random Fourier Features, we can
approximate any function $K$ that can be written in terms of $K_\text{gauss}$.</strong></p>

<h2 id="attention">Attention</h2>

<p>In a <a href="/nlp-from-scratch-annotated-attention/">previous blog post</a>, we went over
the <em>self-attention</em> mechanism, an how it was introduced for language
translation. Nowadays, most language models use <em>scaled-dot-product attention</em> as
defined in the <em>Transformers</em> paper <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>:</p>

\[\text{Attention}(Q, K, V) =
\text{softmax} \left( \frac{QK^\top}{\sqrt{d}}\right)\]

<p>Where $Q, K, V \in \mathbb{R}^{L \times d}$, $L$ is the sequence length, and $d$
is some hidden dimension. We can expand the $\text{softmax}$ and rewrite
the expression as the following:</p>

\[\text{Attention}(Q, K, V) = D^{-1}AV, \quad
A = \exp(QK^\top/\sqrt{d}), \quad
D = \text{diag}(A 1_L)\]

<p>Where $1_L$ is a vector of ones of length $L$. In Python this looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">d</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="n">d_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">d_inv</span> <span class="o">@</span> <span class="n">a</span> <span class="o">@</span> <span class="n">v</span>
</code></pre></div></div>

<p>This $\text{Attention}$ method has some limitations however; if we
examine the attention matrix, $A$, we will realize it is of shape $\mathbb{R}^{L
\times L}$, meaning that any operation performed with $A$ will have a time
and space complexity that grows <strong>quadratically</strong> with respect to the sequence
length $L$. This puts a limitation on the maximum sequence length that can be
used with the Transformer, which means they are not usable for many tasks that
may require much longer sequence lengths, such as dialog, protein sequences, and
images.</p>

<p>Many have developed their own “X-former” in an attempt to reduce this
complexity, for a full survey see <a class="citation" href="#tay2020efficient">(Tay et al., 2020)</a>.</p>

<h2 id="performer">Performer</h2>

<h3 id="softmax-kernel">Softmax Kernel</h3>

<p>The <em>Performer</em> <a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a>  seeks to reduce the
complexity of Attention using random Fourier features. Using the equation above,
we can factor out the normalization component, $\sqrt{d}$, in the
computation of $A$:</p>

\[A = \exp \left(
\frac{Q}{\sqrt[4]{d}}
\left( \frac{K}{\sqrt[4]{d}} \right)^\top
\right)\]

<p>As we continue, we will assume that this normalization is applied beforehand, so
we can rewrite as simply:</p>

\[A = \exp(QK^\top)\]

<p>Now lets say we define a softmax kernel, $K_\text{softmax} : \mathbb{R}^d \times
\mathbb{R}^d \mapsto \mathbb{R}$ as:</p>

\[K_\text{softmax}(x_i, x_j) = \exp(x_i^\top x_j)\]

<p>We can rewrite the computation of $A$ yet again as:</p>

\[A(i, j) = K_\text{softmax}(q_i^\top,k_j^\top)\]

<p>Where $q_i$, $k_j$, represent the $i^\text{th}$,
$j^\text{th}$ row vector in $Q$, $K$, respectively. Representing the attention
matrix as the output of $K_\text{softmax}$ means we can now
approximate it at a <strong>lower dimensionality</strong> using some random feature mapping 
$z: \mathbb{R}^L \mapsto \mathbb{R}^R$:</p>

\[K_{softmax}(x_i, x_j) \approx z(x_i)^\top z(x_j)\]

<p><em>Note: <a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a> uses $\varphi$ to denote this random
feature mapping, I am using $z$ for consistency.</em> Working from the definition of
the Gaussian kernel function, we can derive $K_\text{softmax}$
in terms of $K_\text{gauss}$:</p>

\[K_\text{softmax}(x_i, x_j) =
\exp \left( \frac{\lVert x_i \rVert^2}{2} \right)
K_\text{gauss}(x_i, x_j)
\exp \left( \frac{\lVert x_j \rVert^2}{2} \right)\]

<p>See Appendix TODO for full derivation. With this derivation, it is trivial to
come up with our random feature mapping $z_\omega$ that approximates
the $K_\text{softmax}$ kernel:</p>

\[z_\omega^\text{sin/cos}(x) = 
\exp \left( \frac{\lVert x \rVert^2}{2} \right)
\begin{bmatrix} \cos(\omega^\top x) \\ 
\sin(\omega^\top x) \end{bmatrix}\]

<p>Where, again, $\omega \sim \mathcal{N}_R(0, I)$ is sampled from a spherical
Gaussian. This looks quite complex at this point, but we can now write the full
approximated attention mechanism, $\widehat{\text{Attention}}(Q, K, V)$, in Python like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">z_sin_cos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">):</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">coef</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"...d,rd-&gt;...r"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coef</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">sin</span><span class="p">(</span><span class="n">product</span><span class="p">),</span> <span class="n">cos</span><span class="p">(</span><span class="n">product</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">attention_hat</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">random_dim</span><span class="p">)</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">normalizer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">d</span> <span class="o">**</span> <span class="mf">0.25</span><span class="p">)</span>               <span class="c1"># to normalize before multiplication
</span>    <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">random_dim</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>     <span class="c1"># generate i.i.d. gaussian features
</span>    <span class="n">q_prime</span> <span class="o">=</span> <span class="n">z_sin_cos</span><span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">normalizer</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span> <span class="c1"># apply feature map z to Q
</span>    <span class="n">k_prime</span> <span class="o">=</span> <span class="n">z_sin_cos</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">normalizer</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span> <span class="c1"># apply feature map z to K
</span>    <span class="n">a_hat</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_prime</span> <span class="o">@</span> <span class="n">k_prime</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>              <span class="c1"># approximate attention matrix
</span>    <span class="n">d_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_hat</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)))</span>  <span class="c1"># rest of attention as usual
</span>    <span class="k">return</span> <span class="n">d_inv</span> <span class="o">@</span> <span class="n">a_hat</span> <span class="o">@</span> <span class="n">v</span>
</code></pre></div></div>

<h2 id="orthogonal-random-features">Orthogonal Random Features</h2>

<p>Before we make any comparisons between our approximate and full-rank attention
methods, it is important to mention an additional method introduced in
<a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a>:</p>

<blockquote>
  <p>To further reduce the variance of the estimator (so that we can use even
smaller number of random features $R$), we entangle different random samples $\omega_1, …, \omega_R$
to be exactly orthogonal. This can be done while maintaining
unbiasedness whenever isotropic distributions $D$ are used (i.e. in particular in
all kernels we considered so far) by standard Gram-Schmidt renormalization
procedure.</p>
</blockquote>

<p>See Proof of Theorem 2 in section F.4 of the appendix in <a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a> for the proof that orthogonal random features can improve estimation.</p>

<h2 id="iid-vs-orthogonal-random-features">IID vs. Orthogonal Random Features</h2>

<p>Using $L = 1024$, $d = 16$, we will vary the number of random features $R$
and measure the mean-squared-error (MSE) of our estimated attention and the
full-rank attention.</p>

<p><img src="/images/iid_vs_ortho.png" alt="iid vs ortho" /></p>

<p><em>Lines are mean of 15 samples, shaded region is the standard deviation. All of the
code to reproduce these figures can be found at:
<a href="https://github.com/teddykoker/performer">github.com/teddykoker/performer</a>.</em></p>

<h2 id="positive-random-features">Positive Random Features</h2>

<p><a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a> note that the random feature map
$z^\text{sin/cos}$ can yield negative values, especially when the kernel
outputs approach 0. This is very common for pairs with no interaction, so it can lead
to instability in the estimation. To get around this, they propose a new random
feature map, $z^\text{positive}$:</p>

\[z_\omega^\text{positive}(x) = 
\exp \left(- \frac{\lVert x \rVert^2}{2} \right)
\begin{bmatrix} \exp(\omega^\top x) \end{bmatrix}\]

<p>Written in Python as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">z_positive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">):</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"...d,rd-&gt;...r"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coef</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
</code></pre></div></div>

<p>If we compare $z^\text{positive}$ to $z^\text{sin/cos}$, using
orthogonal features $\omega$, we can see the improvement:</p>

<p><img src="/images/trig_vs_positive.png" alt="trig_vs_positive" /></p>

<p>We find that, as was theoretically proven in the paper, the $z^\text{positive}$
feature map with random orthogonal features yields a strong estimation
of the full-rank attention mechanism, with a time and space complexity that only
grows <strong>linearly</strong> with respect to sequence length $L$. Ultimately, <em>Performers</em> seem to
be a strong approach to reducing the complexity of Transformers, and show
potential to be used in many different sub-fields of deep learning.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Special thanks to Richard Song of Google AI for providing details around some of
the experimentation in the paper.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="choromanski2020rethinking">Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., &amp; others. (2020). Rethinking Attention with Performers. <i>ArXiv Preprint ArXiv:2009.14794</i>.</span></li>
<li><span id="rahimi2007random">Rahimi, A., &amp; Recht, B. (2007). Random features for large-scale kernel machines. <i>Advances in Neural Information Processing Systems</i>, <i>20</i>, 1177–1184.</span></li>
<li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <i>Advances in Neural Information Processing Systems</i>, 5998–6008.</span></li>
<li><span id="tay2020efficient">Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient transformers: A survey. <i>ArXiv Preprint ArXiv:2009.06732</i>.</span></li></ol>

<h2 id="appendix">Appendix</h2>

<p>TODO</p>


  </div><a class="u-url" href="/2020/11/performers/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Teddy Koker</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Teddy Koker</li><li><a class="u-email" href="mailto:teddy.koker@gmail.com">teddy.koker@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">teddykoker</span></a></li><li><a href="https://www.linkedin.com/in/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">teddykoker</span></a></li><li><a href="https://www.twitter.com/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">teddykoker</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Algorithmic Trading and Machine Learning.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
